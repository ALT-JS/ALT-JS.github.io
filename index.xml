<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>JiashenDu Homepage</title>
    <link>https://alt-js.github.io/</link>
      <atom:link href="https://alt-js.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>JiashenDu Homepage</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://alt-js.github.io/media/icon_hu49328279b356d807a3fbed97c60db0cd_36689_512x512_fill_lanczos_center_3.png</url>
      <title>JiashenDu Homepage</title>
      <link>https://alt-js.github.io/</link>
    </image>
    
    <item>
      <title>Example Talk</title>
      <link>https://alt-js.github.io/talk/example-talk/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://alt-js.github.io/talk/example-talk/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Hugo Blox Builder&amp;rsquo;s &lt;a href=&#34;https://docs.hugoblox.com/reference/content-types/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://docs.hugoblox.com/reference/markdown/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including &lt;a href=&#34;https://docs.hugoblox.com/reference/markdown/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page elements&lt;/a&gt; such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DREAMoR: Diffusion-based REconstruction And Motion prioR</title>
      <link>https://alt-js.github.io/project/dreamor/</link>
      <pubDate>Thu, 01 May 2025 00:00:00 +0000</pubDate>
      <guid>https://alt-js.github.io/project/dreamor/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Are LLMs complicated ethical dilemma analyzers?</title>
      <link>https://alt-js.github.io/project/ethicallm/</link>
      <pubDate>Tue, 01 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://alt-js.github.io/project/ethicallm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>COMPSCI 280 Project3: Facial Keypoint Detection</title>
      <link>https://alt-js.github.io/post/280pj3/</link>
      <pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://alt-js.github.io/post/280pj3/</guid>
      <description>&lt;p&gt;url: &lt;a href=&#34;https://alt-js.github.io/post/280pj3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://alt-js.github.io/post/280pj3&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;part-1-direct-coordinate-regression&#34;&gt;Part 1: Direct Coordinate Regression&lt;/h2&gt;
&lt;h3 id=&#34;implementation-details&#34;&gt;Implementation details&lt;/h3&gt;
&lt;p&gt;I&amp;rsquo;m using SmoothL1Loss for the &lt;code&gt;SimpleNet&lt;/code&gt; training(the weights is from SmoothL1Loss) and learning rate 5e-4. This SimpleNet model is a straightforward convolutional neural network designed for facial keypoint detection. It takes a single-channel 224×224 grayscale image as input and applies a series of four convolutional blocks—each block consisting of convolution, batch normalization, ReLU activation, max pooling, and dropout—to progressively extract and compress features, reducing the image size down to roughly 13×13. The resulting feature map is then flattened and passed through three fully connected layers, with the final layer outputting 136 values corresponding to 68 (x, y) keypoints. The network also uses Xavier initialization for the FC layers to improve convergence during training.&lt;/p&gt;
&lt;h3 id=&#34;training-and-validation-loss-curves&#34;&gt;Training and validation loss curves&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/08/K1vQBlsoh52maUi.png&#34; alt=&#34;simplenet_losses.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;visualizations-on-test-images&#34;&gt;Visualizations on test images&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/08/pGUVMcb8WazNLRO.png&#34; alt=&#34;smoothl1_smoothl1_epoch20lr5e-4.png.jpg.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;pros-and-cons&#34;&gt;Pros and Cons&lt;/h3&gt;
&lt;p&gt;SimpleNet is easy to design and fast to run, but it doesn&amp;rsquo;t converge well and the result isn&amp;rsquo;t OK to use in real applications because the keypoints aren&amp;rsquo;t real to be actual human faces.&lt;/p&gt;
&lt;h2 id=&#34;part-2-transfer-learning-for-keypoint-detection&#34;&gt;Part 2: Transfer Learning for Keypoint Detection&lt;/h2&gt;
&lt;h3 id=&#34;implementation-details-1&#34;&gt;Implementation details&lt;/h3&gt;
&lt;p&gt;This model is a modified ResNet-18 adapted for facial keypoint detection on grayscale images. It takes a 1-channel image as input by replacing the first convolutional layer with a new one that accepts a single channel, and its weights are initialized by averaging the pretrained RGB weights. The final fully connected layer is replaced with a new linear layer that outputs 136 values corresponding to 68 (x, y) keypoints. Additionally, the model supports an optional parameter to freeze the backbone for transfer learning, ensuring that only the final regression head is trained if desired. I set 10 epoches for the first stage where the pretrained backbone is frozen and only train the new layers, and I finetuned the whole model for another 10 epoches where all the parameters are active. Both stages has a learning rate of 5e-4.&lt;/p&gt;
&lt;h3 id=&#34;training-and-validation-loss-curves-1&#34;&gt;Training and validation loss curves&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/08/2LhuxcdXrBVPQ6z.png&#34; alt=&#34;resnet18_losses.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;visualizations-on-test-images-1&#34;&gt;Visualizations on test images&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/08/lZWFai5UfYvue6X.png&#34; alt=&#34;resnet18lr5e-4result.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;pros-and-cons-1&#34;&gt;Pros and Cons&lt;/h3&gt;
&lt;p&gt;The Resnet18 yields better performance compared to the previous SimpleNet and thanks to utilizing existing pretrained backbone, it&amp;rsquo;s not too hard to design the network. Although it costs more time compared to the SimpleNet, its results show way less flaws and I would say it&amp;rsquo;s applicable for natural human face keypoint detection.&lt;/p&gt;
&lt;h2 id=&#34;part-3-heatmap-based-keypoint-detection&#34;&gt;Part 3: Heatmap-based Keypoint Detection&lt;/h2&gt;
&lt;h3 id=&#34;implementation-details-2&#34;&gt;Implementation details&lt;/h3&gt;
&lt;p&gt;This UNet model is an encoder-decoder architecture designed for keypoint heatmap prediction, taking a single-channel image and outputting 68 heatmaps. In the encoder (or &amp;ldquo;down&amp;rdquo; part), the network applies several sequential convolutional blocks—each consisting of two convolution layers with batch normalization and ReLU activation—to progressively extract features while preserving spatial information, and uses max pooling to reduce spatial resolution after each block, storing intermediate outputs as skip connections. The bottleneck further processes the compressed representation with a deeper convolutional block that doubles the feature channels, while the decoder (or &amp;ldquo;up&amp;rdquo; part) upsamples the bottleneck output using transposed convolutions and concatenates it with the corresponding skip connections (after resizing if needed), followed by additional convolutional blocks to refine the combined features. Finally, a 1×1 convolution projects the features to the desired number of output channels (68), with each channel representing the predicted heatmap for a facial keypoint.&lt;/p&gt;
&lt;p&gt;For the keypoint extraction function, it extracts keypoint coordinates from predicted heatmaps by processing a batch of heatmaps with shape [B, K, H, W], where B is the batch size, K is the number of keypoints, and H and W are the heatmap height and width. For each sample in the batch and for each keypoint channel, it flattens the heatmap into a 1D tensor and uses argmax to find the index of the highest activation, which is assumed to correspond to the keypoint location. The index is then converted back into two-dimensional coordinates by computing the row (y-coordinate) as the quotient of the index divided by W and the column (x-coordinate) as the remainder of that division. The coordinates for each keypoint are collected per sample and ultimately returned as a tensor containing all predicted keypoints.&lt;/p&gt;
&lt;h3 id=&#34;training-and-validation-loss&#34;&gt;Training and validation loss&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/08/d79SocqibMyhERX.png&#34; alt=&#34;unet_losses.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;visualizations-on-test-images-2&#34;&gt;Visualizations on test images&lt;/h3&gt;
&lt;p&gt;Click to zoom in.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/08/pTgfLisr3oGyCmO.png&#34; alt=&#34;unet9.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/08/Us8xBlbumpzjeVc.png&#34; alt=&#34;unet19.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/08/OarnzLF3c5IE41V.png&#34; alt=&#34;unet14.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/08/1yu8ftV53hBjmEG.png&#34; alt=&#34;unet20.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;pros-and-cons-2&#34;&gt;Pros and Cons&lt;/h3&gt;
&lt;p&gt;The UNet result is very satisfying and it converges really fast, you can get decent workable model in 2~3 epoches, and its validation loss is also low. The cons is that UNet in my approach is sensitive to learning rate, in my test 1e-4 yields nice and consistent results.&lt;/p&gt;
&lt;h3 id=&#34;discussion&#34;&gt;Discussion&lt;/h3&gt;
&lt;p&gt;I started out to have really random and bad results with the heatmaps and the keypoints extracted from the keypoints. I showed my picture results to one of my friend and he said, &amp;ldquo;Your UNet looks like an edge detector.&amp;rdquo; I thought about it intuitively, the sigma in my heatmap, it was 1 originally. If the heatmap only has sigma=1, the heatmap can only cover a relatively small amount of area in the picture, and the feature it extracts will probably be an edge, that&amp;rsquo;s probably why it looked like an edge detector. So I cranked up the sigma to 2.5(not too big), and the UNet started to behave very nice.&lt;/p&gt;
&lt;h2 id=&#34;misc&#34;&gt;Misc&lt;/h2&gt;
&lt;p&gt;Here are three model weights:
&lt;a href=&#34;https://drive.google.com/file/d/10GPoXDNmRQ9vKUgeF9YZ5Y3g_5lE5i6m/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://drive.google.com/file/d/10GPoXDNmRQ9vKUgeF9YZ5Y3g_5lE5i6m/view?usp=sharing&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/18SkoUGENJywnrIW2g3LVnK3OxS9jDQdo/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://drive.google.com/file/d/18SkoUGENJywnrIW2g3LVnK3OxS9jDQdo/view?usp=sharing&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1IVIEIwCkIZ618gGq4wSG24NfUtl2SNEA/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://drive.google.com/file/d/1IVIEIwCkIZ618gGq4wSG24NfUtl2SNEA/view?usp=sharing&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here is the &lt;code&gt;.ipynb&lt;/code&gt; file:
&lt;a href=&#34;https://drive.google.com/file/d/1PMTGhzRGHqDZ11OWpXvznUgYt07QMsHT/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://drive.google.com/file/d/1PMTGhzRGHqDZ11OWpXvznUgYt07QMsHT/view?usp=sharing&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>COMPSCI 184 Project4: ClothSim</title>
      <link>https://alt-js.github.io/post/184pj4/</link>
      <pubDate>Tue, 18 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://alt-js.github.io/post/184pj4/</guid>
      <description>&lt;h2 id=&#34;part-0-project-overview&#34;&gt;Part 0: Project overview&lt;/h2&gt;
&lt;p&gt;In this project, I&amp;rsquo;ve implemented a real-time simulation of cloth using a mass and spring based system. Started out with masses and springs, I build grid of point masses and implemented 3 type of constraints(structual, shearing and bending) presented as strings between the point masses. Next, I implemented easy simulation with classic Newton&amp;rsquo;s law F = ma, Hooke&amp;rsquo;s law for forces on masses, verlet integration for new point mass positions and Provot&amp;rsquo;s study to ensure spring’s length is at most 10% greater than its &lt;code&gt;rest_length&lt;/code&gt; at the end of any time step. Then it&amp;rsquo;s the collision handling with spheres and flat surfaces. Followed with self-collision handling which makes the falling fabric more realistic so that it won&amp;rsquo;t overlap unnaturally. Finally, I use GLSL to implement a lot of different vertex shaders and fragment shaders: diffuse shading, blinn-phong shading, texture mapping, bump mapping, displacement mapping and mirror-like environment-mapped reflection.&lt;/p&gt;
&lt;h2 id=&#34;part-1-masses-and-springs&#34;&gt;Part 1: Masses and springs&lt;/h2&gt;
&lt;h3 id=&#34;some-screenshots-of-scenepinned2json&#34;&gt;Some screenshots of &lt;code&gt;scene/pinned2.json&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;You can also treat the picture w/ all constraints below as the third view.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/04/215ldLmxnXsfibC.png&#34; alt=&#34;1-1-1.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/04/vLzrByYgEcFb23j.png&#34; alt=&#34;1-1-2.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;View 1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;View 2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;the-wireframe-screenshots&#34;&gt;The wireframe screenshots&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/04/wqOYRpJmDEfknsr.png&#34; alt=&#34;1-2-1.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/04/4aYcWV2ZCKPwNvR.png&#34; alt=&#34;1-2-2.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/04/kC4spg9feZozAqI.png&#34; alt=&#34;1-2-3.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;w/ all constraints&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;w/o shearing&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;w/ only shearing&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;part-2-simulation-via-numerical-integration&#34;&gt;Part 2: Simulation via numerical integration&lt;/h2&gt;
&lt;h3 id=&#34;experiments-with-some-the-parameters-in-the-simulation&#34;&gt;Experiments with some the parameters in the simulation&lt;/h3&gt;
&lt;h4 id=&#34;spring-constant-ks&#34;&gt;Spring constant &lt;code&gt;ks&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;When the &lt;code&gt;ks&lt;/code&gt; is very small, the cloth behaves very &amp;ldquo;loose&amp;rdquo; and tiny little force will make the whole cloth moves and &amp;ldquo;shivers&amp;rdquo;. When the &lt;code&gt;ks&lt;/code&gt; is very big, the cloth behaves very &amp;ldquo;tight&amp;rdquo; and sags less than smaller &lt;code&gt;ks&lt;/code&gt;, the whole cloth will reach to a stop faster than smaller &lt;code&gt;ks&lt;/code&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/04/6JRKixjeml2TrCu.png&#34; alt=&#34;2-1-1-1.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/04/XHTtclnERh2UK7p.png&#34; alt=&#34;2-1-1-2.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;code&gt;ks&lt;/code&gt; = 50000&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;code&gt;ks&lt;/code&gt; = 5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;density&#34;&gt;Density&lt;/h4&gt;
&lt;p&gt;With higher density, the cloth sags more than lower density when at full stop, also, I noticed that when they are dropping down, higher the density, slower the speed, and also the deformation in the middle of the cloth is bigger. This is because the density cause the point mass to be heavier/lighter.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/04/g3rLvHh8Okt9cxR.png&#34; alt=&#34;2-1-2-1.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/04/ai2uZwXFcqyNKrY.png&#34; alt=&#34;2-1-2-2.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/04/9aOSn3HQV8iNdKJ.png&#34; alt=&#34;2-1-2-3.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;code&gt;density&lt;/code&gt; = 100&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;og &lt;code&gt;density&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;code&gt;density&lt;/code&gt; = 1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;damping&#34;&gt;Damping&lt;/h3&gt;
&lt;p&gt;Bigger damping value results in much stable movement when falling down, which indicates that the delta of point masses location doesn&amp;rsquo;t change dramatically. From the picture, very low damping value results in the cloth behaves jumping around actively, whereas higher damping value initially controls the cloth movement and then even the sagging speed is lower.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/04/sR5v4x3A82d6TND.png&#34; alt=&#34;2-1-3-1.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/04/ZAILVijTPWM592k.png&#34; alt=&#34;2-1-3-2.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;big &lt;code&gt;damping&lt;/code&gt; value&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;super small &lt;code&gt;damping&lt;/code&gt; value&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;scenepinned4json-results&#34;&gt;&lt;code&gt;scene/pinned4.json&lt;/code&gt; results&lt;/h3&gt;
&lt;p&gt;All default.
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/04/AUt2nq1DMR38fQy.png&#34; alt=&#34;2-2.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;part-3-handling-collisions-with-other-objects&#34;&gt;Part 3: Handling collisions with other objects&lt;/h2&gt;
&lt;h3 id=&#34;implementation-of-collisions&#34;&gt;Implementation of collisions&lt;/h3&gt;
&lt;h4 id=&#34;sphere-collisions&#34;&gt;Sphere collisions&lt;/h4&gt;
&lt;p&gt;I first calculate the distance between the pointmass and origin of the sphere&amp;rsquo;s surface, if it&amp;rsquo;s landing on it then we just return, if not, I compute the correction vector from the tangent (&lt;code&gt;origin + (dir / dist) * radius&lt;/code&gt;) minus last position, scaled down by friction.&lt;/p&gt;
&lt;h4 id=&#34;plane-collisions&#34;&gt;Plane collisions&lt;/h4&gt;
&lt;p&gt;The function first computes the signed distances of the point mass&amp;rsquo;s last and current positions from the plane by taking the dot product of the difference between the positions and a reference point on the plane with the plane&amp;rsquo;s normal. If the product of these distances is negative, it indicates that the point mass has crossed the plane. The function then calculates the fraction along the point mass&amp;rsquo;s trajectory where the collision occurs and determines the exact collision point (referred to as &amp;ldquo;tangent&amp;rdquo;) on the plane. Depending on whether the point mass was above or below the plane, it adjusts the collision point by a small offset to avoid numerical issues. Finally, it computes a correction vector that accounts for friction, updating the point mass&amp;rsquo;s position so that it lies on or near the plane after the collision.&lt;/p&gt;
&lt;h3 id=&#34;scenespherejson-results&#34;&gt;&lt;code&gt;scene/sphere.json&lt;/code&gt; results&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/IbWiJdGH2ukl9tT.png&#34; alt=&#34;3-2-1.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/tUvQ6sYWhTZxPe7.png&#34; alt=&#34;3-2-2.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/bn75WBKiOlXAgdv.png&#34; alt=&#34;3-2-3.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;code&gt;ks&lt;/code&gt; = 500&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;og &lt;code&gt;ks&lt;/code&gt;(&lt;code&gt;ks&lt;/code&gt; = 5000)&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;code&gt;ks&lt;/code&gt; = 50000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The higher the &lt;code&gt;ks&lt;/code&gt;, the cloth seems more &amp;ldquo;extended&amp;rdquo; and harder to deform, which matches what &lt;code&gt;ks&lt;/code&gt; contributes to the property of the cloth.&lt;/p&gt;
&lt;h3 id=&#34;cloth-on-plane-result&#34;&gt;Cloth on plane result&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/l6SYxOJ9bV8qHpj.png&#34; alt=&#34;3-3.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;part-4-handling-self-collisions&#34;&gt;Part 4: Handling self-collisions&lt;/h2&gt;
&lt;h3 id=&#34;implementation-of-self-coliision&#34;&gt;Implementation of self-coliision&lt;/h3&gt;
&lt;p&gt;To implement self-collision for the cloth, I used a spatial hashing approach to efficiently detect and resolve collisions between point masses. First, the &lt;code&gt;build_spatial_map&lt;/code&gt; function creates a spatial map by iterating over all point masses and hashing their positions into discrete grid cells using the &lt;code&gt;hash_position&lt;/code&gt; function, which computes a unique float identifier for each 3D box volume based on the cloth dimensions. Then, in the &lt;code&gt;self_collide&lt;/code&gt; function, for a given point mass, I retrieve nearby candidate point masses from the spatial map (i.e., those that share the same hash key). For each candidate (excluding the point mass itself), I calculate the distance between their positions; if it is less than twice the cloth thickness, I compute a corrective displacement vector to push them apart. The average of these correction vectors, scaled by the simulation step size, is then added to the point mass’s position, effectively resolving the self-collision.&lt;/p&gt;
&lt;h3 id=&#34;self-collision-at-different-time&#34;&gt;Self-collision at different time&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/ZA2kTGuvW74jbE6.png&#34; alt=&#34;4-2-1.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/1xPNJ9CA5zIUs7u.png&#34; alt=&#34;4-2-2.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/vaDemBTCKixV1Ib.png&#34; alt=&#34;4-2-3.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;early stage&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;middle stage&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;near final stage&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;varied-density-and-ks&#34;&gt;Varied &lt;code&gt;density&lt;/code&gt; and &lt;code&gt;ks&lt;/code&gt;&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/mRgIblpESi8ry1w.png&#34; alt=&#34;4-3-1-1.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/vdAn2seEBlFZoNp.png&#34; alt=&#34;4-3-1-2.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;og &lt;code&gt;ks&lt;/code&gt;(&lt;code&gt;ks&lt;/code&gt; = 5000)&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;code&gt;ks&lt;/code&gt; = 50000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Smaller &lt;code&gt;ks&lt;/code&gt; makes the cloth &amp;ldquo;softer&amp;rdquo; and there will be more overlapping and creases when cloth falls on itself.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/mRgIblpESi8ry1w.png&#34; alt=&#34;4-3-1-1.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/Xuk2NDYpP7vEFfs.png&#34; alt=&#34;4-3-2-1.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;og &lt;code&gt;density&lt;/code&gt;(15)&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;code&gt;density&lt;/code&gt; = 5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Smaller &lt;code&gt;density&lt;/code&gt; makes the cloth more smooth and &amp;ldquo;flat&amp;rdquo; when it falls on itself. (Because smaller density causes smaller pointmass weight and individual pointmass won&amp;rsquo;t fall into the cloth that easily.)&lt;/p&gt;
&lt;h2 id=&#34;part-5-shaders&#34;&gt;Part 5: Shaders&lt;/h2&gt;
&lt;h3 id=&#34;shader-program-vertex--fragment-shaders&#34;&gt;Shader program, vertex &amp;amp; fragment shaders&lt;/h3&gt;
&lt;p&gt;A shader program is a set of instructions that runs on the GPU to determine how graphics are rendered on the screen. It typically includes different types of shaders, such as vertex and fragment shaders, that work together to produce the final image.&lt;/p&gt;
&lt;p&gt;The vertex shader is the first stage in the rendering pipeline. It processes each vertex of a 3D model, transforming its position from object space into screen space. It can also compute additional data like normals, texture coordinates, and lighting information per vertex.&lt;/p&gt;
&lt;p&gt;The fragment shader comes later in the pipeline and operates on each pixel (or fragment) that results from the rasterization of the transformed vertices. It takes the interpolated data produced by the vertex shader and uses it to determine the final color of each pixel by applying textures, lighting models, and material properties. By combining the computations from both shaders, a shader program creates realistic lighting and material effects, making objects appear as if they interact with light in a lifelike manner.&lt;/p&gt;
&lt;h3 id=&#34;the-blinn-phong-shading-model&#34;&gt;The Blinn-Phong shading model&lt;/h3&gt;
&lt;p&gt;The Blinn-Phong shading model breaks down the lighting into three main components: ambient light(the constant, background light in a scene that uniformly illuminates all surfaces), diffuse reflection(models how light scatters off a rough surface, depending on the angle between the surface normal and the light direction) and specular reflection.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/PHXxcq3ZALkysQB.png&#34; alt=&#34;5-2-1.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/by3Ei9OIsZ4LrjP.png&#34; alt=&#34;5-2-2.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;only ambient&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;only diffuse&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/sQ3AKiOpXZUCu8T.png&#34; alt=&#34;5-2-3.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/aGrlcjIUQfvknRF.png&#34; alt=&#34;5-2-4.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;only specular&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;entire Blinn-Phong&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;texture-mapping-shader&#34;&gt;Texture mapping shader&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/NYV6xZpXhd15RUJ.png&#34; alt=&#34;5-3.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/vYeS4capTL7PQEt.png&#34; alt=&#34;5-3-2.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;render result&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;custom texture&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;bump-mapping-and-displacement-mapping&#34;&gt;Bump mapping and displacement mapping&lt;/h3&gt;
&lt;h4 id=&#34;some-results&#34;&gt;Some results&lt;/h4&gt;
&lt;p&gt;normal = 2, height = 1&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/MVHK96Gvrfq2IQW.png&#34; alt=&#34;5-4-1-1.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/6mdcTl8AHZhXONj.png&#34; alt=&#34;5-4-1-2.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;bump mapping cloth&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;bump mapping sphere&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;normal = 100 and height = 0.02&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/bwCY69HpJRMLkao.png&#34; alt=&#34;5-4-1-3.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/Y3GfbWqgvJaEFA7.png&#34; alt=&#34;5-4-1-4.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;displacement mapping cloth&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;displacement mapping sphere&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;comparison&#34;&gt;Comparison&lt;/h4&gt;
&lt;p&gt;Bump Mapping:
Bump mapping simulates surface detail by perturbing the surface normals used in lighting calculations, which creates the illusion of texture without modifying the actual geometry. In the screenshots, the cloth shows subtle wrinkles and folds that enhance its fabric appearance, while the sphere exhibits texture detail (such as bumps or grooves) on its surface. However, since the underlying geometry remains unchanged, the silhouette and shadow edges of both objects are not affected by these details.&lt;/p&gt;
&lt;p&gt;Displacement Mapping:
Displacement mapping, on the other hand, alters the actual geometry by moving vertices along the normal direction based on the texture’s intensity values. In the displacement mapping screenshot of the sphere, you can see more pronounced surface details that not only affect the shading but also modify the object&amp;rsquo;s silhouette and the way shadows are cast. This results in a more realistic depiction of rough or uneven surfaces, at the cost of additional computational overhead.&lt;/p&gt;
&lt;h4 id=&#34;sphere-mesh-coarseness-effect&#34;&gt;Sphere mesh coarseness effect&lt;/h4&gt;
&lt;p&gt;Above: coarse(&lt;code&gt;-o 16 -a 16&lt;/code&gt;), below: fine(&lt;code&gt;-o 128 -a 128&lt;/code&gt;).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/q9Kwgi4L1Qr5zav.png&#34; alt=&#34;5-4-3-1.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/ObvRm31YPCdyKfS.png&#34; alt=&#34;5-4-3-3.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/tWcMwrvkTJjNxGb.png&#34; alt=&#34;5-4-3-2.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/au9LkxdCGsQYBpf.png&#34; alt=&#34;5-4-3-4.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;bump mapping&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;displacement mapping&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The bump mapping behaves consistant from coarse to fine, and the displacement mapping sphere is more realistic when it&amp;rsquo;s fine compared to coarse.&lt;/p&gt;
&lt;h3 id=&#34;mirror-shader&#34;&gt;Mirror shader&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/VaIdpc1bUlYE57T.png&#34; alt=&#34;5-5-1.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/04/06/d2UEImafqR5xOoB.png&#34; alt=&#34;5-5-2.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;mirror shader on cloth&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;mirror shader on sphere&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>COMPSCI 280 Project2: Flow Matching</title>
      <link>https://alt-js.github.io/post/280pj2/</link>
      <pubDate>Sun, 09 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://alt-js.github.io/post/280pj2/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;You will train your own flow matching model on MNIST.&lt;/p&gt;
&lt;h2 id=&#34;part-1-training-a-single-step-denoising-unet&#34;&gt;Part 1: Training a Single-Step Denoising UNet&lt;/h2&gt;
&lt;h3 id=&#34;unconditioned-unet-architecture&#34;&gt;Unconditioned UNet architecture&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cal-cs180.github.io/fa24/hw/proj5/assets/unconditional_arch.png&#34; alt=&#34;fig1&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cal-cs180.github.io/fa24/hw/proj5/assets/atomic_ops_new.png&#34; alt=&#34;fig2&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;visualization-of-the-noising-process&#34;&gt;Visualization of the noising process&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/14/VapRt2jWhLcIC5m.png&#34; alt=&#34;fig3.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;training-loss-curve&#34;&gt;Training loss curve&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/14/Xo6LJGaAh3bKsNF.png&#34; alt=&#34;fig4.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;sample-results-on-the-test-set-after-the-first-and-the-5-th-epoch&#34;&gt;Sample results on the test set after the first and the 5-th epoch&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/14/1JgCnLkd5ZFaTeu.png&#34; alt=&#34;fig5.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/14/7K12zJM4ox6Icm8.png&#34; alt=&#34;fig6.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;sample-results-on-the-test-set-with-out-of-distribution-noise-levels-after-the-model-is-trained&#34;&gt;Sample results on the test set with out-of-distribution noise levels after the model is trained.&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/13/tzSuKb4QPysqL7A.png&#34; alt=&#34;fig7.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;sample-results-on-the-test-set-with-pure-noise&#34;&gt;Sample results on the test set with pure noise&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/13/2v94YdzxhlALSi7.png&#34; alt=&#34;fig8.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;average-image-of-the-training-set&#34;&gt;Average image of the training set&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/13/kI5rCa3uQsLbUoR.png&#34; alt=&#34;fig9.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

The average MNIST image is blurry and lacks clear digits, showing general intensity distributions. The denoised results, however, retain distinct shapes, indicating that it is doing denoising.&lt;/p&gt;
&lt;h2 id=&#34;part-2-training-a-flow-matching-model&#34;&gt;Part 2: Training a Flow Matching Model&lt;/h2&gt;
&lt;h3 id=&#34;training-loss-curve-plot-for-the-time-conditioned-unet-over-the-whole-training-process&#34;&gt;Training loss curve plot for the time-conditioned UNet over the whole training process&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/14/HaQGujkX1vgocIb.png&#34; alt=&#34;fig10_tunet.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;sampling-results-for-the-time-conditioned-unet-for-5-and-10-epochs&#34;&gt;Sampling results for the time-conditioned UNet for 5 and 10 epochs&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/14/JPRa2lU9YkINCFo.png&#34; alt=&#34;tunet_sample.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;training-loss-curve-plot-for-the-class-conditioned-unet-over-the-whole-training-process&#34;&gt;Training loss curve plot for the class-conditioned UNet over the whole training process&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/14/dxosKPJYICX9Tki.png&#34; alt=&#34;fig10_cunet.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;sampling-results-for-the-class-conditioned-unet-for-5-and-10-epochs&#34;&gt;Sampling results for the class-conditioned UNet for 5 and 10 epochs&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/14/62mLjKZDs7nkih5.png&#34; alt=&#34;cunet_sample.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>COMPSCI 184 Project3: PathTracer</title>
      <link>https://alt-js.github.io/post/184pj3/</link>
      <pubDate>Tue, 04 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://alt-js.github.io/post/184pj3/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/18/eHbOGFcswjBzxDa.png&#34; alt=&#34;dragon_64_32.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;part-0-project-overview&#34;&gt;Part 0: Project overview&lt;/h2&gt;
&lt;p&gt;In this project, I implemented some fundamental functions and features of a physically-based renderer using path tracing algorithm. First, I implemented ray generation and the intersection between ray and objects/scene. Next, I implemented the BVH splitting and intersection detection. Then, I implemented direct illumination(zero/one bounce lights) and global illumination(multi-bounce lights w/ w/o russian roullete) to actually render the whole scene with path tracing. Finally, I implemented adaptive sampling to reduce the noise of monte carlo path tracing.&lt;/p&gt;
&lt;h2 id=&#34;part-1-ray-generation-and-scene-intersection&#34;&gt;Part 1: Ray Generation and Scene Intersection&lt;/h2&gt;
&lt;h3 id=&#34;ray-generation-walkthrough--primitive-intersection-pipeline&#34;&gt;Ray generation walkthrough &amp;amp; primitive intersection pipeline&lt;/h3&gt;
&lt;p&gt;I first calculate the bottom left and the top right corner of the sensor/image space coordinates, which helps me to determine the coordinates in the camera space with left multiplcation of the &lt;code&gt;c2w&lt;/code&gt; matrix. Then with the origin &lt;code&gt;pos&lt;/code&gt; and the previously calculated coord as &lt;code&gt;direction&lt;/code&gt; vector(since the origin is [0,0,0] and it&amp;rsquo;s numerically the same for the coords and the vector), we can generate the correct ray.&lt;/p&gt;
&lt;p&gt;Primitive is the bridge between geometry processing and the shading subsystem. It contains all the things including &lt;code&gt;bbox&lt;/code&gt;, &lt;code&gt;intersect&lt;/code&gt;, &lt;code&gt;has_intersection&lt;/code&gt; and &lt;code&gt;bsdf&lt;/code&gt;. For intersections, we use &lt;code&gt;has_intersection&lt;/code&gt; to determine whether it&amp;rsquo;s intersected or not and use &lt;code&gt;intersect&lt;/code&gt; to actually change the intersection distance, the normal vector, and the bsdf.&lt;/p&gt;
&lt;h3 id=&#34;triangle-intersection-algorithm-walkthrough&#34;&gt;Triangle intersection algorithm walkthrough&lt;/h3&gt;
&lt;p&gt;For the &lt;code&gt;intersect&lt;/code&gt; and &lt;code&gt;has_intersection&lt;/code&gt;, I used the Moller-Trumbore algorithm to compute them. We compute two edges of the triangle. Compute the determinant &lt;code&gt;det&lt;/code&gt; to check if the ray is nearly parallel. Compute u and v(barycentric coordinates) to ensure the intersection lies within the triangle. Compute t, the intersection distance along the ray. And finally check if t is within the ray&amp;rsquo;s valid range (min_t to max_t). The function &lt;code&gt;intersect&lt;/code&gt; basically determine whether &lt;code&gt;has_intersection&lt;/code&gt; is true or not and change the primitives and the properties mentioned above(intersection distance, normal vector, bsdf).&lt;/p&gt;
&lt;h3 id=&#34;some-results&#34;&gt;Some results&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/17/PSxngs9r1DGBQvh.png&#34; alt=&#34;1-CBspheres.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/17/ahoUQ67VMJ8Hnm5.png&#34; alt=&#34;1-cow.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;CB spheres&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;cow&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;part-2-bounding-volume-hierarchy&#34;&gt;Part 2: Bounding Volume Hierarchy&lt;/h2&gt;
&lt;h3 id=&#34;bvh-construction-algorithm-walkthrough&#34;&gt;BVH construction algorithm walkthrough&lt;/h3&gt;
&lt;p&gt;First, compute the bounding box (BBox) of all primitives in the given range [start, end). Then, I choose the best axis to split: compute the bounding box centroid for each primitive. Select the axis with the largest extent (X, Y, or Z) and split at the centroid median to achieve balanced partitions. The primitives are sorted first and partitioned into left and right subsets. Finally, recursively call function and construct BVH for left and right subsets and return the newly created BVHNode.&lt;/p&gt;
&lt;h3 id=&#34;some-results-1&#34;&gt;Some results&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/17/hsbaMc4D8Z1iKzq.png&#34; alt=&#34;2-cblucy.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/17/h2aK4RpOl9yntiw.png&#34; alt=&#34;2-max.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;CB Lucy&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Max Planck&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;more-on-bvh-acceleration&#34;&gt;More on BVH acceleration&lt;/h3&gt;
&lt;p&gt;First, here&amp;rsquo;s more result on the BVH rendering(left) and the comparison between the non-BVH. You can see that there&amp;rsquo;s basically no difference between them.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/17/L7pVYQdUeayEkjW.png&#34; alt=&#34;2-peter.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/17/sKw7S8VnQ9bU3k2.png&#34; alt=&#34;2slow-peter.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Peter w/ BVH&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Peter w/o BVH&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;But the rendering time, as shown in the picture below, have significant difference. One is 0.0653 second, and the other takes 166.5520 seconds to finish. Nearly 300x faster.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/17/mGnP5FgaLyU4qWM.png&#34; alt=&#34;2-peterspeed.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;part-3-direct-illumination&#34;&gt;Part 3: Direct Illumination&lt;/h2&gt;
&lt;h3 id=&#34;direct-lighting-functions-walkthrough&#34;&gt;Direct Lighting functions walkthrough&lt;/h3&gt;
&lt;h4 id=&#34;uniform-hemisphere-sampling&#34;&gt;Uniform Hemisphere Sampling&lt;/h4&gt;
&lt;p&gt;In the loop of &lt;code&gt;num_sample&lt;/code&gt; samples, we sample from &lt;code&gt;hemisphereSampler&lt;/code&gt;, then we create a new ray with in point &lt;code&gt;hit_p&lt;/code&gt; and the epsilon constant for avoiding numerical presision issues and direction sampled earlier. If the ray intersects we calculate the &lt;code&gt;L_out&lt;/code&gt; of the ray. Finally we divide the sum of all the things above with pdf and the &lt;code&gt;num_samples&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&#34;importance-sampling-lights&#34;&gt;Importance Sampling Lights&lt;/h4&gt;
&lt;p&gt;FOr importance sampling, we sample all the lights directly After we calculate the hit point &lt;code&gt;hit_p&lt;/code&gt;, we directly sample a light ray from it. If we cast a ray in this direction and there is no other object between the hit point and the light source, then we know that this light source does cast light onto the hit point. After the judge, we still do the same thing as above(divide the sum of all the things above with pdf and the &lt;code&gt;num_samples&lt;/code&gt;).&lt;/p&gt;
&lt;h3 id=&#34;some-results-2&#34;&gt;Some results&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/17/dcW1BYZT6Jpvswl.png&#34; alt=&#34;3H-bunny_64_32.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/17/uhZSVJynjMpz6UK.png&#34; alt=&#34;3-bunny_64_32.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Hemisphere uniform sampling&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Direct importance sampling&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/17/sbQr1iKAhxGZSg7.png&#34; alt=&#34;3H-dragon_32_16.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/17/olEq7FRBQMzStYb.png&#34; alt=&#34;3-dragon_32_16.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Hemisphere uniform sampling&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Direct importance sampling&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;direct-comparison&#34;&gt;Direct comparison&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/17/pd3KXycOaBsR8Eo.png&#34; alt=&#34;3-1-bunny_64_32.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/17/ORMdsz56EyBwQFS.png&#34; alt=&#34;3-4-bunny_64_32.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/17/wuqkp8aj7XcZ3PR.png&#34; alt=&#34;3-16-bunny_64_32.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/17/5exniCTFvOcGjm9.png&#34; alt=&#34;3-64-bunny_64_32.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Up 1 Down 16&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Up 4 Down 64&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Uniform hemisphere sampling distributes samples evenly over the hemisphere, leading to significant variance in Monte Carlo integration when evaluating lighting contributions, especially in scenes with strong directional lighting. This results in more noise and slower convergence. In contrast, importance sampling strategically places more samples in directions where the light source contributes most to the radiance, reducing variance and improving efficiency. This method converges faster, producing smoother and more accurate results with fewer samples.&lt;/p&gt;
&lt;h2 id=&#34;part-4-global-illumination&#34;&gt;Part 4: Global Illumination&lt;/h2&gt;
&lt;h3 id=&#34;indirect-lighting-function-walkthough&#34;&gt;Indirect lighting function walkthough&lt;/h3&gt;
&lt;p&gt;We get the one bounce radiance first, then, we sample a new f using the outgoing direction of the last ray. We create new ray and increase its depth by 1. When the depth reaches the max depth, just return the one bounce result earlier(this is the last one being calculated), otherwise recursively calling the function.&lt;/p&gt;
&lt;p&gt;As for russian roulette, the difference is that every new ray generated each turn has to pass the russian roulette coin flip check in order to not be eliminated and thrown away. The probability I&amp;rsquo;m using here is 70% continue, 30% delete each turn.&lt;/p&gt;
&lt;h3 id=&#34;some-global-illumination-results&#34;&gt;Some global illumination results&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/17/D4Ye7gfVZy5TcUq.png&#34; alt=&#34;4-bunny_1024_8_2.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/17/bfBkY6LQGyCnXAZ.png&#34; alt=&#34;4-spheres_1024_8_4.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Bunny (maxdepth=2)&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Spheres (maxdepth=4)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;only-direct-illumination-vs-only-indirect-illumination&#34;&gt;Only direct illumination vs. only indirect illumination&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/17/QzaAcpTOljU9CMK.png&#34; alt=&#34;4-spheres_1024_8_1.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/18/4fCLUGcaYm8dwqI.png&#34; alt=&#34;4-spheres_1024_8_2onlyi.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;only direct illumination&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;only indirect illumination (maxdepth=2)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;renders-of-max_ray_depth-from-0-to-5&#34;&gt;Renders of &lt;code&gt;max_ray_depth&lt;/code&gt; from 0 to 5&lt;/h3&gt;
&lt;p&gt;specs using: &lt;code&gt;-t 12 -s 1024 -l 8 -r 480 360&lt;/code&gt;, click the image to zoom in on the website.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;isAccumBounces&lt;/th&gt;
&lt;th&gt;m = 0&lt;/th&gt;
&lt;th&gt;m = 1&lt;/th&gt;
&lt;th&gt;m = 2&lt;/th&gt;
&lt;th&gt;m = 3&lt;/th&gt;
&lt;th&gt;m = 4&lt;/th&gt;
&lt;th&gt;m = 5&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/18/cGERHpAguQeCFLI.png&#34; alt=&#34;CB_Bunny_0.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/18/IYqgA2rjBJsduiV.png&#34; alt=&#34;CB_Bunny_1.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/18/OnDZbmFq9UihCgL.png&#34; alt=&#34;CB_Bunny_2.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/18/sqfh6ZUnmKetLET.png&#34; alt=&#34;CB_Bunny_3.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/18/r1iAbHDxV48nmp3.png&#34; alt=&#34;CB_Bunny_4.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/18/8X7aYmyHVGTpEQs.png&#34; alt=&#34;CB_Bunny_5.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/18/bvizX7BDAYJN89T.png&#34; alt=&#34;CB_Bunny_cumsum_0.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/18/VjzShtWMLb7gIQy.png&#34; alt=&#34;CB_Bunny_cumsum_1.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/18/3WxsAPvbGEq6HNL.png&#34; alt=&#34;CB_Bunny_cumsum_2.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/18/LeQf9sqC3vta2hG.png&#34; alt=&#34;CB_Bunny_cumsum_3.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/18/epknVAIzZwdN2Ly.png&#34; alt=&#34;CB_Bunny_cumsum_4.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/18/ts5jVlDz1NhGSZF.png&#34; alt=&#34;CB_Bunny_cumsum_5.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;the-2nd-and-3rd-bounce-of-light&#34;&gt;the 2nd and 3rd bounce of light&lt;/h4&gt;
&lt;p&gt;For the 2nd bounce of the light, its main contribution is that it illuminates the ceiling, making the overall rasterization way more realistic. For the third bounce of light, it slightly illuminates the ground around the bunny, mimicing the lights reflected away from the bunny surface, making the overall rasterization more realistic.&lt;/p&gt;
&lt;h3 id=&#34;russian-roulette-rendering-results&#34;&gt;Russian roulette rendering results&lt;/h3&gt;
&lt;p&gt;specs using: &lt;code&gt;-s 1024 -l 4&lt;/code&gt;. Along with using russian soulette, you may not be able to see that much difference after 2 bounce.&lt;/p&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;meta charset=&#34;UTF-8&#34;&gt;
    &lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, initial-scale=1.0&#34;&gt;
    &lt;title&gt;Flexbox&lt;/title&gt;
    &lt;style&gt;
        .container1 {
            display: flex;
            justify-content: space-between; /* 图片之间有间距 */
            align-items: center;
            gap: 10px; /* 图片间距 */
        }
        .container1 img {
            width: 100px; /* 设定图片宽度 */
            height: auto; /* 保持宽高比 */
        }
    &lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;div class=&#34;container1&#34;&gt;
        &lt;img src=&#34;https://s2.loli.net/2025/03/18/ter3DLVmPMNxOHU.png&#34; alt=&#34;图片1&#34;&gt;
        &lt;img src=&#34;https://s2.loli.net/2025/03/18/4bXiQIOSD6k2h9G.png&#34; alt=&#34;图片2&#34;&gt;
        &lt;img src=&#34;https://s2.loli.net/2025/03/18/f2LITNAPtaVv6QO.png&#34; alt=&#34;图片3&#34;&gt;
        &lt;img src=&#34;https://s2.loli.net/2025/03/18/y4ZuJkjoRhrLM5e.png&#34; alt=&#34;图片4&#34;&gt;
        &lt;img src=&#34;https://s2.loli.net/2025/03/18/8PTfcBJsKSzVAYa.png&#34; alt=&#34;图片5&#34;&gt;
        &lt;img src=&#34;https://s2.loli.net/2025/03/18/ys6JOa3PBA4E7Zo.png&#34; alt=&#34;图片5&#34;&gt;
    &lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;h3 id=&#34;various-sample-per-pixel-rates-results&#34;&gt;Various sample-per-pixel rates results&lt;/h3&gt;
&lt;p&gt;From left to right, from top to bottom: 1 2 4 8 16 64 1024.&lt;/p&gt;
&lt;head&gt;
    &lt;meta charset=&#34;UTF-8&#34;&gt;
    &lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, initial-scale=1.0&#34;&gt;
    &lt;title&gt;Flexbox&lt;/title&gt;
    &lt;style&gt;
        .container2 {
            display: flex;
            justify-content: space-between; /* 图片之间有间距 */
            align-items: center;
            gap: 10px; /* 图片间距 */
        }
        .container2 img {
            width: 150px; /* 设定图片宽度 */
            height: auto; /* 保持宽高比 */
        }
    &lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;div class=&#34;container2&#34;&gt;
        &lt;img src=&#34;https://s2.loli.net/2025/03/18/fHB89v74NbV1RAe.png&#34; alt=&#34;图片1&#34;&gt;
        &lt;img src=&#34;https://s2.loli.net/2025/03/18/a7iHhyqIOwczQ3C.png&#34; alt=&#34;图片2&#34;&gt;
        &lt;img src=&#34;https://s2.loli.net/2025/03/18/IZNlfs51XEhi8Pe.png&#34; alt=&#34;图片3&#34;&gt;
        &lt;img src=&#34;https://s2.loli.net/2025/03/18/DGeFtJuWlV6I9Sd.png&#34; alt=&#34;图片4&#34;&gt;
    &lt;/div&gt;
&lt;/body&gt;
&lt;head&gt;
    &lt;meta charset=&#34;UTF-8&#34;&gt;
    &lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, initial-scale=1.0&#34;&gt;
    &lt;title&gt;Flexbox&lt;/title&gt;
    &lt;style&gt;
        .container3 {
            display: flex;
            justify-content: space-between; /* 图片之间有间距 */
            align-items: center;
            gap: 10px; /* 图片间距 */
        }
        .container3 img {
            width: 200px; /* 设定图片宽度 */
            height: auto; /* 保持宽高比 */
        }
    &lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;div class=&#34;container3&#34;&gt;
        &lt;img src=&#34;https://s2.loli.net/2025/03/18/tdysICTwuG6kzU1.png&#34; alt=&#34;图片1&#34;&gt;
        &lt;img src=&#34;https://s2.loli.net/2025/03/18/gn7KfalOB6sDwiJ.png&#34; alt=&#34;图片2&#34;&gt;
        &lt;img src=&#34;https://s2.loli.net/2025/03/18/nhL8oIT5qeFlKXN.png&#34; alt=&#34;图片3&#34;&gt;
    &lt;/div&gt;
&lt;/body&gt;
&lt;h2 id=&#34;part-5-adaptive-sampling&#34;&gt;Part 5: Adaptive Sampling&lt;/h2&gt;
&lt;h3 id=&#34;adaptive-sampling-walkthrough&#34;&gt;Adaptive sampling walkthrough&lt;/h3&gt;
&lt;p&gt;Adaptive sampling is a method used in ray tracing to reduce the number of samples per pixel while maintaining image quality. Instead of taking a fixed number of samples (ns_aa), it dynamically decides when to stop sampling based on statistical analysis.&lt;/p&gt;
&lt;p&gt;(If only one sample is requested, the algorithm traces a single ray and directly stores the computed radiance.) We loop over &lt;code&gt;ns_aa&lt;/code&gt; samples, and only check after every &lt;code&gt;samplesPerBatch&lt;/code&gt; samples. Then we calculate and check confidence interval &lt;code&gt;I&lt;/code&gt; is smaller or larger than a threshold, if smaller then it stops early. otherwise just like ordinary calculation. Once sampling is complete (either because the pixel converged or the maximum samples were taken), the final radiance is computed as the average of all collected samples.&lt;/p&gt;
&lt;h3 id=&#34;some-results-3&#34;&gt;Some results&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/18/SIEatGOiRCykVZY.png&#34; alt=&#34;5-bunny.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/18/Jy58qPUbscnmpvN.png&#34; alt=&#34;5-bunnyrate.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;bunny&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;bunnyrate&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/18/hwvjBs8Wyx4Dk2G.png&#34; alt=&#34;5-ball.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/18/3SGViozB81c6rlH.png&#34; alt=&#34;5-ballrate.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ball&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ballrate&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>COMPSCI 184 Project2: Geometric Modeling</title>
      <link>https://alt-js.github.io/post/184pj2/</link>
      <pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://alt-js.github.io/post/184pj2/</guid>
      <description>&lt;h2 id=&#34;part-0-project-overview&#34;&gt;Part 0: Project overview&lt;/h2&gt;
&lt;p&gt;This homework focuses on implementing fundamental operations related to Bezier curves, Bezier patches, mesh manipulation, and Loop subdivision. In this project, we started out from the basic Bezier curve and de Casteljau algorithm, and extend it to 2D surfaces. We also made a fully functional mesh &amp;ldquo;editor&amp;rdquo; that does edge flipping, edge splitting and mesh upsampling. The most challenging parts were ensuring correct pointer assignments in the edge splitting and properly handling &lt;code&gt;newEdge&lt;/code&gt; in Loop subdivision.&lt;/p&gt;
&lt;h1 id=&#34;section-1-bezier-curves-and-surfaces&#34;&gt;Section 1: Bezier Curves and Surfaces&lt;/h1&gt;
&lt;h2 id=&#34;part-1-bezier-curves-with-1d-de-casteljau-subdivision&#34;&gt;Part 1: Bezier Curves with 1D de Casteljau Subdivision&lt;/h2&gt;
&lt;h3 id=&#34;briefly-explain-de-casteljaus-algorithm-and-how-you-implemented-it-in-order-to-evaluate-bezier-curves&#34;&gt;Briefly explain de Casteljau’s algorithm and how you implemented it in order to evaluate Bezier curves.&lt;/h3&gt;
&lt;p&gt;De Casteljau&amp;rsquo;s algorithm is used to determine the point on the Bezier curve of a certain number of points. For $n$ points, we iteratively use linear interpolation to determine $n-1$ points, until we only have one point, which is $p_{\frac{n(n-1)}{2}}$. This particular point will be on the Bezier curve of $p_{0}$ and $p_{n-1}$ from the original $n$ points.&lt;/p&gt;
&lt;p&gt;I wrote a seperate &lt;code&gt;lerp&lt;/code&gt; function and iterate it through all the points. The &lt;code&gt;lerp&lt;/code&gt; function is defined as:



$$\text{lerp}(p_i, p_{i+1}, t) = (1 - t) p_i + t p_{i + 1}$$

The new points generated is the one-step result of all the original points.&lt;/p&gt;
&lt;h3 id=&#34;the-bezier-curve-with-6-control-points&#34;&gt;The Bezier curve with 6 control points&lt;/h3&gt;
&lt;p&gt;The six pictures below shows the 5 steps of evaluation of the original 6 control points and the final bezier of the points.&lt;/p&gt;
&lt;div style=&#34;display: grid; grid-template-columns: repeat(2, 2fr); gap: 10px;&#34;&gt;
    &lt;img src=&#34;https://s2.loli.net/2025/02/27/piCm81GaKkMHJDt.png&#34; width=&#34;300&#34;&gt;
    &lt;img src=&#34;https://s2.loli.net/2025/02/27/wkpT9gBd8HcjNEI.png&#34; width=&#34;300&#34;&gt;
    &lt;img src=&#34;https://s2.loli.net/2025/02/27/fIQkrm3ZJBbVzWE.png&#34; width=&#34;300&#34;&gt;
    &lt;img src=&#34;https://s2.loli.net/2025/02/27/3SFZzB69aqbUkuW.png&#34; width=&#34;300&#34;&gt;
    &lt;img src=&#34;https://s2.loli.net/2025/02/27/OAn197glD8jyLNY.png&#34; width=&#34;300&#34;&gt;
    &lt;img src=&#34;https://s2.loli.net/2025/02/27/wixVUPGJHTMAmCo.png&#34; width=&#34;300&#34;&gt;
&lt;/div&gt;
&lt;p&gt;In this picture I changed the position of the points and the $t$ value.


















&lt;figure  id=&#34;figure-a-slightly-different-bezier-curve&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/02/27/dWjQbgScHh7a3PX.png&#34; alt=&#34;A slightly different Bezier curve&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      A slightly different Bezier curve
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;part-2-bezier-surfaces-with-separable-1d-de-casteljau&#34;&gt;Part 2: Bezier Surfaces with Separable 1D de Casteljau&lt;/h2&gt;
&lt;h3 id=&#34;briefly-explain-how-de-casteljau-algorithm-extends-to-bezier-surfaces-and-how-you-implemented-it-in-order-to-evaluate-bezier-surfaces&#34;&gt;Briefly explain how de Casteljau algorithm extends to Bezier surfaces and how you implemented it in order to evaluate Bezier surfaces.&lt;/h3&gt;
&lt;p&gt;The de Casteljau&amp;rsquo;s algorithm naturally extends to Bezier surfaces by applying the same interpolation process in two directions $u$ and $v$. For each row of control points $P(i,j)$, apply the de Casteljau algorithm recursively to compute an intermediate set of points at parameter $u$, reducing the row into a single point. Once you have the intermediate points from the previous step, apply de Casteljau along the $v$-direction to interpolate these points and obtain the final surface point $S(u,v)$.&lt;/p&gt;
&lt;p&gt;I modified the &lt;code&gt;lerp&lt;/code&gt; function to make it accept &lt;code&gt;Vector3D&lt;/code&gt; inputs and outputs, and the &lt;code&gt;evaluate&lt;/code&gt; process is just call &lt;code&gt;evaluate1D&lt;/code&gt; on controlPoints, and return with the results of the previous loop being run in the &lt;code&gt;evaluate1D&lt;/code&gt; again but with a different interpolation parameter. The &lt;code&gt;evaluate1D&lt;/code&gt; function also loops through points with &lt;code&gt;evaluateStep&lt;/code&gt; but only returns the 0-th dimension of the result.&lt;/p&gt;
&lt;h3 id=&#34;the-teapot-result&#34;&gt;The teapot result&lt;/h3&gt;


















&lt;figure  id=&#34;figure-the-screenshot-of-bezteapotbez-evaluated-by-my-implementation&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/02/27/HfmGqn4UWvBTpbP.png&#34; alt=&#34;the screenshot of `bez/teapot.bez` evaluated by my implementation&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      the screenshot of &lt;code&gt;bez/teapot.bez&lt;/code&gt; evaluated by my implementation
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 id=&#34;part-3-area-weighted-vertex-normals&#34;&gt;Part 3: Area-Weighted Vertex Normals&lt;/h2&gt;
&lt;h3 id=&#34;briefly-explain-how-you-implemented-the-area-weighted-vertex-normals&#34;&gt;Briefly explain how you implemented the area-weighted vertex normals.&lt;/h3&gt;
&lt;p&gt;Started with one halfedge, we iterate through every halfedge. For every halfedge, we extract the previous, current, next vertex position, and calculate the cross product of vectors defined by current and next vertex and current and previous vertex. We add up all the cross products and restore it to unit vector as the output.&lt;/p&gt;
&lt;h3 id=&#34;the-teapot-result-1&#34;&gt;The teapot result&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/01/AJwZbInO9fBehc2.png&#34; alt=&#34;3-flat.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/01/mblVCkjNiun8sXJ.png&#34; alt=&#34;3-phong.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;default flat shading&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Phong shading&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;part-4-edge-flip&#34;&gt;Part 4: Edge Flip&lt;/h2&gt;
&lt;h3 id=&#34;briefly-explain-how-you-implemented-the-edge-flip-operation&#34;&gt;Briefly explain how you implemented the edge flip operation.&lt;/h3&gt;
&lt;p&gt;I first determine every possible element of a unit pair of triangles, then, I changed the relations of every possible related halfedge using &lt;code&gt;setNeighbors&lt;/code&gt; function, and assign the faces and vertices with the correct halfedge in the end. Here&amp;rsquo;s a picture that contains all the notations I&amp;rsquo;m using in my code.


















&lt;figure  id=&#34;figure-flip-edge-draft&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/01/WrGMPZszA5bd8ap.png&#34; alt=&#34;Flip edge draft&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Flip edge draft
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;the-teapot-result-2&#34;&gt;The teapot result&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/01/gFQa8CGeAH4lYMD.png&#34; alt=&#34;4-original.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/01/bNqGmDOSWnVrEh9.png&#34; alt=&#34;4-flipped.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;The original teapot&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;The teapot woth some flipped edges&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;the-freakindebug-journey&#34;&gt;The freakin&amp;rsquo;debug journey&lt;/h3&gt;
&lt;p&gt;From the draft, you probably noticed that I initially wrote the vertex $c$ and $d$ in the wrong place. Because I sticked to my draft so closely, I kept checking the errors in my code, not the draft itself:(.&lt;/p&gt;
&lt;h2 id=&#34;part-5-edge-split&#34;&gt;Part 5: Edge Split&lt;/h2&gt;
&lt;h3 id=&#34;briefly-explain-how-you-implemented-the-edge-split-operation&#34;&gt;Briefly explain how you implemented the edge split operation&lt;/h3&gt;
&lt;p&gt;I first did all the things similar to the &lt;code&gt;filpEdge&lt;/code&gt; function(determine possible elements), then I created new edge, face and halfedges. For the new edgeHere’s a picture that contains all the notations I’m using in my code.


















&lt;figure  id=&#34;figure-split-edge-draft&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/01/K3AQNaltcrGRmJw.png&#34; alt=&#34;Split edge draft&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Split edge draft
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;some-teapot-result&#34;&gt;Some teapot result&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/01/3tdvjYxD7clyfSe.png&#34; alt=&#34;5-original.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/01/hsd3pgJiZX4mkTY.png&#34; alt=&#34;5-split.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/01/EenLAPbxRMaXrFf.png&#34; alt=&#34;5-splitflip.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;The original teapot&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;The teapot with some split edges&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;The teapot with some splits and filpped edges&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;part-6-loop-subdivision-for-mesh-upsampling&#34;&gt;Part 6: Loop Subdivision for Mesh Upsampling&lt;/h2&gt;
&lt;h3 id=&#34;briefly-explain-how-you-implemented-the-loop-subdivision&#34;&gt;Briefly explain how you implemented the loop subdivision&lt;/h3&gt;
&lt;p&gt;I first compute new positions for all the vertices in the input mesh, using the weighted average formula given in the question. Mark each vertex as being a vertex of the original mesh, and compute the updated vertex positions associated with edges new positions. Then loop through the mesh to split edges and setting new halfedges generated, as well as filp any new edge that connects an old and new vertex to make the mesh more &amp;ldquo;organized&amp;rdquo;. Finally we copy the new vertex positions into &lt;code&gt;Vertex::position&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;how-meshes-behave-after-loop-subdivision&#34;&gt;How meshes behave after loop subdivision?&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/01/EcgSYzGNr3Aho2j.png&#34; alt=&#34;6-1.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/01/cMblmfDSrW1AyGE.png&#34; alt=&#34;6-2.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/01/pxUJqVXGBuF6vYh.png&#34; alt=&#34;6-3.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/01/nyoKM7lk3mDSjJF.png&#34; alt=&#34;6-4.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Meshes becomes &amp;ldquo;smoothed out&amp;rdquo; after multiple times of loop subdivisions. Sharp edges and corners become more rounded and less sharper than before.&lt;/p&gt;
&lt;p&gt;Yes, pre-splitting some edges works. Loop subdivision applies a weighted averaging scheme when repositioning vertices. If edges are closely spaced(i.e. pre-splitting to make edges closely spaced / evenly spaced), the averaging effect is localized rather than spread out over a large area, reducing the smoothing effect.&lt;/p&gt;
&lt;h3 id=&#34;the-asymmetric-cube&#34;&gt;The Asymmetric Cube&lt;/h3&gt;
&lt;p&gt;Asymmetry appears after multiple iterations, despite the cube starting with a symmetric structure. This happens due to the cube&amp;rsquo;s original topology, where each face consists of two triangles (not four evenly distributed ones).&lt;/p&gt;
&lt;p&gt;Loop subdivision refines each triangle individually, meaning that if some edges are longer or unevenly split, the algorithm treats them differently and there&amp;rsquo;s asymmetric. The cube&amp;rsquo;s initial triangulation pattern isn&amp;rsquo;t perfectly symmetrical, leading to small &amp;ldquo;drifts&amp;rdquo; after each iteration.&lt;/p&gt;
&lt;p&gt;So, by pre-splitting the cube by adding a &amp;ldquo;cut&amp;rdquo; to each surface(formed by 2 triangles/faces), we are able to get a balanced cube. Because every surface are pre-splitted to be exactly the same and symmetrical, so after subdivision, they still remain the same shape and still be symmetrical on same directions.&lt;/p&gt;


















&lt;figure  id=&#34;figure-the-pre-split-cube&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/01/nO1Lz28iMFSTbZG.png&#34; alt=&#34;The pre-split cube&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The pre-split cube
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Here is a comparison between original cube and the pre-splitted cube after 4 times of subdivisions.(Left: og. Right: pre-splitted.)&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/01/nyoKM7lk3mDSjJF.png&#34; alt=&#34;6-4.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.loli.net/2025/03/01/FAwiQjGgW4HKBJP.png&#34; alt=&#34;6-comparison.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>COMPSCI 184 Project1: The Simple Rasterizer</title>
      <link>https://alt-js.github.io/post/184pj1/</link>
      <pubDate>Tue, 04 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://alt-js.github.io/post/184pj1/</guid>
      <description>&lt;embed src=&#34;CS184PJ1Writeup.pdf&#34; type=&#34;application/pdf&#34; width=&#34;100%&#34; height=&#34;600px&#34; /&gt;
</description>
    </item>
    
    <item>
      <title>Foundations of Computer Graphics</title>
      <link>https://alt-js.github.io/post/compsci184/</link>
      <pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://alt-js.github.io/post/compsci184/</guid>
      <description>&lt;h2 id=&#34;cs-184284a-spring-2025&#34;&gt;CS 184/284A Spring 2025&lt;/h2&gt;
&lt;!-- ![](https://cs184.eecs.berkeley.edu/sp25/assets/images/logo.png) --&gt;
&lt;h2 id=&#34;contents&#34;&gt;Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://alt-js.github.io/post/184pj1/&#34;&gt;Project 1: The Simple Rasterizer&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://alt-js.github.io/post/184pj2/&#34;&gt;Project 2: Geometric Modeling&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://alt-js.github.io/post/184pj3/&#34;&gt;Project 3: PathTracer&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://alt-js.github.io/post/184pj4/&#34;&gt;Project 4: ClothSim&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://zx40224617.github.io/CS184_Final_Deliverables/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Final Project: VR hand interactive deformation&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Graduate Computer Vision</title>
      <link>https://alt-js.github.io/post/compsci280/</link>
      <pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://alt-js.github.io/post/compsci280/</guid>
      <description>&lt;h2 id=&#34;cs-c280-spring-2025&#34;&gt;CS C280 Spring 2025&lt;/h2&gt;
&lt;h2 id=&#34;contents&#34;&gt;Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://alt-js.github.io/post/280pj2/&#34;&gt;COMPSCI 280 Project2: Flow Matching&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://alt-js.github.io/post/280pj3/&#34;&gt;COMPSCI 280 Project3: Facial Keypoint Detection&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://jingfeng0705.github.io/DREAMoR2/dreamor.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COMPSCI 280 Final Project: DREAMoR&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>How GPT learn layer by layer</title>
      <link>https://alt-js.github.io/project/layerbylayer/</link>
      <pubDate>Wed, 01 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://alt-js.github.io/project/layerbylayer/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Zen</title>
      <link>https://alt-js.github.io/project/zen/</link>
      <pubDate>Sun, 10 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://alt-js.github.io/project/zen/</guid>
      <description></description>
    </item>
    
    <item>
      <title>I’M HOI: Inertia-aware Monocular Capture of 3D Human-Object Interactions</title>
      <link>https://alt-js.github.io/publication/conference-paper/imhoi/</link>
      <pubDate>Sat, 18 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://alt-js.github.io/publication/conference-paper/imhoi/</guid>
      <description>&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
&lt;!-- Add the publication&#39;s **full text** or **supplementary notes** here. You can use rich formatting such as including [code, math, and images](https://docs.hugoblox.com/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://alt-js.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://alt-js.github.io/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-hugo-blox-builder&#34;&gt;Create slides in Markdown with Hugo Blox Builder&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://hugoblox.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo Blox Builder&lt;/a&gt; | &lt;a href=&#34;https://docs.hugoblox.com/content/slides/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://revealjs.com/pdf-export/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;porridge&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;blueberry&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;porridge&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;blueberry&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Eating...&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% fragment %}} One {{% /fragment %}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% fragment %}} **Two** {{% /fragment %}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% fragment %}} Three {{% /fragment %}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
  One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  Three
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% speaker_note %}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;-&lt;/span&gt; Only the speaker can read these notes
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;-&lt;/span&gt; Press &lt;span class=&#34;sb&#34;&gt;`S`&lt;/span&gt; key to view
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  {{% /speaker_note %}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;background-image&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;/media/boards.jpg&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;background-color&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;#0000FF&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;my-style&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-css&#34; data-lang=&#34;css&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h3&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;color&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;navy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://discord.gg/z8wNYzb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.hugoblox.com/content/slides/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
